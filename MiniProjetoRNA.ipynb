{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Parte 1 - código inicial"
      ],
      "metadata": {
        "id": "8vXRG8dP84in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    epsilon = 1e-8\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y_true * np.log(y_pred))\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2 / self.input_size)\n",
        "        self.weights2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2 / self.hidden_size)\n",
        "\n",
        "        self.bias1 = np.zeros((1, self.hidden_size))\n",
        "        self.bias2 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def train(self, X, y, epochs, batch_size):\n",
        "        num_examples = X.shape[0]\n",
        "        iterations_per_epoch = num_examples // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for _ in range(iterations_per_epoch):\n",
        "                indices = np.random.choice(num_examples, size=batch_size, replace=False)\n",
        "                X_batch = X[indices]\n",
        "                y_batch = y[indices]\n",
        "\n",
        "                hidden_layer = relu(np.dot(X_batch, self.weights1) + self.bias1)\n",
        "                output_layer = softmax(np.dot(hidden_layer, self.weights2) + self.bias2)\n",
        "\n",
        "                output_error = y_batch - output_layer\n",
        "                output_delta = output_error / batch_size\n",
        "\n",
        "                hidden_error = output_delta.dot(self.weights2.T)\n",
        "                hidden_delta = hidden_error * relu_derivative(hidden_layer)\n",
        "\n",
        "                self.weights2 += hidden_layer.T.dot(output_delta) * self.learning_rate\n",
        "                self.weights1 += X_batch.T.dot(hidden_delta) * self.learning_rate\n",
        "\n",
        "                self.bias2 += np.sum(output_delta, axis=0) * self.learning_rate\n",
        "                self.bias1 += np.sum(hidden_delta, axis=0) * self.learning_rate\n",
        "\n",
        "    def predict(self, X):\n",
        "        hidden_layer = relu(np.dot(X, self.weights1) + self.bias1)\n",
        "        output_layer = softmax(np.dot(hidden_layer, self.weights2) + self.bias2)\n",
        "        return np.argmax(output_layer, axis=1)\n",
        "\n",
        "# Carregamento e pré-processamento dos dados MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784) / 255.0\n",
        "X_test = X_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "y_train_encoded = np.zeros((y_train.shape[0], 10))\n",
        "y_train_encoded[np.arange(y_train.shape[0]), y_train] = 1\n",
        "\n",
        "# Definição e treinamento da MLP\n",
        "mlp = MLP(input_size=784, hidden_size=20, output_size=10, learning_rate=0.01)\n",
        "mlp.train(X_train, y_train_encoded, epochs=500, batch_size=128)\n",
        "\n",
        "# Avaliação da MLP\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Acurácia: {:.2%}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxLTKbQKgWMy",
        "outputId": "bbef770f-62d2-445e-a86b-5fa37392dab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Acurácia: 96.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Parte* 2 - Propagação para frente e custo total"
      ],
      "metadata": {
        "id": "vnYiLBv688k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    epsilon = 1e-8\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y_true * np.log(y_pred))\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2 / self.input_size)\n",
        "        self.weights2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2 / self.hidden_size)\n",
        "\n",
        "        self.bias1 = np.zeros((1, self.hidden_size))\n",
        "        self.bias2 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        hidden_layer = relu(np.dot(X, self.weights1) + self.bias1)\n",
        "        output_layer = softmax(np.dot(hidden_layer, self.weights2) + self.bias2)\n",
        "        return hidden_layer, output_layer\n",
        "\n",
        "    def train(self, X, y, epochs, batch_size):\n",
        "        num_examples = X.shape[0]\n",
        "        iterations_per_epoch = num_examples // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for _ in range(iterations_per_epoch):\n",
        "                indices = np.random.choice(num_examples, size=batch_size, replace=False)\n",
        "                X_batch = X[indices]\n",
        "                y_batch = y[indices]\n",
        "\n",
        "                hidden_layer, output_layer = self.forward_propagation(X_batch)\n",
        "\n",
        "                output_error = y_batch - output_layer\n",
        "                output_delta = output_error / batch_size\n",
        "\n",
        "                hidden_error = output_delta.dot(self.weights2.T)\n",
        "                hidden_delta = hidden_error * relu_derivative(hidden_layer)\n",
        "\n",
        "                self.weights2 += hidden_layer.T.dot(output_delta) * self.learning_rate\n",
        "                self.weights1 += X_batch.T.dot(hidden_delta) * self.learning_rate\n",
        "\n",
        "                self.bias2 += np.sum(output_delta, axis=0) * self.learning_rate\n",
        "                self.bias1 += np.sum(hidden_delta, axis=0) * self.learning_rate\n",
        "\n",
        "            _, output = self.forward_propagation(X)\n",
        "            cost = cross_entropy_loss(y, output)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Cost: {cost}\")\n",
        "\n",
        "        total_cost = cross_entropy_loss(y, output)\n",
        "        print(f\"Total Cost: {total_cost}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        _, output = self.forward_propagation(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "# Loading and preprocessing the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784) / 255.0\n",
        "X_test = X_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "y_train_encoded = np.zeros((y_train.shape[0], 10))\n",
        "y_train_encoded[np.arange(y_train.shape[0]), y_train] = 1\n",
        "\n",
        "# Definição e treinamento da MLP\n",
        "mlp = MLP(input_size=784, hidden_size=20, output_size=10, learning_rate=0.1)\n",
        "mlp.train(X_train, y_train_encoded, epochs=200, batch_size=256)\n",
        "\n",
        "# Avaliação da MLP\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Acurácia: {:.2%}\".format(accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT4niI1_8924",
        "outputId": "86d45b74-31f3-4060-f633-8406b9ae4868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Cost: 0.03820118865101651\n",
            "Epoch 2/200, Cost: 0.031598174759672575\n",
            "Epoch 3/200, Cost: 0.028100633449656954\n",
            "Epoch 4/200, Cost: 0.0260332992716032\n",
            "Epoch 5/200, Cost: 0.024956272735033803\n",
            "Epoch 6/200, Cost: 0.023502431167181044\n",
            "Epoch 7/200, Cost: 0.022808797441382957\n",
            "Epoch 8/200, Cost: 0.022179515969556522\n",
            "Epoch 9/200, Cost: 0.021008645527420836\n",
            "Epoch 10/200, Cost: 0.020431844805786605\n",
            "Epoch 11/200, Cost: 0.01984965341959259\n",
            "Epoch 12/200, Cost: 0.01939972323461476\n",
            "Epoch 13/200, Cost: 0.018536304869566356\n",
            "Epoch 14/200, Cost: 0.018140966618163903\n",
            "Epoch 15/200, Cost: 0.017703443182053337\n",
            "Epoch 16/200, Cost: 0.017592549940986506\n",
            "Epoch 17/200, Cost: 0.01684748886462961\n",
            "Epoch 18/200, Cost: 0.01656082622653447\n",
            "Epoch 19/200, Cost: 0.016168510325960863\n",
            "Epoch 20/200, Cost: 0.01574516557957164\n",
            "Epoch 21/200, Cost: 0.015548941119180605\n",
            "Epoch 22/200, Cost: 0.015304459159185081\n",
            "Epoch 23/200, Cost: 0.014833517916971375\n",
            "Epoch 24/200, Cost: 0.01467613945654769\n",
            "Epoch 25/200, Cost: 0.014681064880935292\n",
            "Epoch 26/200, Cost: 0.014654793517067874\n",
            "Epoch 27/200, Cost: 0.013891450781326042\n",
            "Epoch 28/200, Cost: 0.013703843586175098\n",
            "Epoch 29/200, Cost: 0.013496694208302282\n",
            "Epoch 30/200, Cost: 0.013226305604182942\n",
            "Epoch 31/200, Cost: 0.013125471758590737\n",
            "Epoch 32/200, Cost: 0.012926509303441984\n",
            "Epoch 33/200, Cost: 0.012838385303972591\n",
            "Epoch 34/200, Cost: 0.012605794663737922\n",
            "Epoch 35/200, Cost: 0.012534334189567337\n",
            "Epoch 36/200, Cost: 0.012293751423275016\n",
            "Epoch 37/200, Cost: 0.012256442348461955\n",
            "Epoch 38/200, Cost: 0.012251374420534583\n",
            "Epoch 39/200, Cost: 0.012080375096552071\n",
            "Epoch 40/200, Cost: 0.01187256194366454\n",
            "Epoch 41/200, Cost: 0.011714601381222985\n",
            "Epoch 42/200, Cost: 0.011580384479316\n",
            "Epoch 43/200, Cost: 0.011324703321895032\n",
            "Epoch 44/200, Cost: 0.011274266294495934\n",
            "Epoch 45/200, Cost: 0.01165373971228076\n",
            "Epoch 46/200, Cost: 0.01118272965580565\n",
            "Epoch 47/200, Cost: 0.010916004010764124\n",
            "Epoch 48/200, Cost: 0.011197870010726467\n",
            "Epoch 49/200, Cost: 0.01067349274572466\n",
            "Epoch 50/200, Cost: 0.01060269355536706\n",
            "Epoch 51/200, Cost: 0.010573346736686915\n",
            "Epoch 52/200, Cost: 0.01053124644053521\n",
            "Epoch 53/200, Cost: 0.010495039841972024\n",
            "Epoch 54/200, Cost: 0.010362507364318341\n",
            "Epoch 55/200, Cost: 0.01016065374771317\n",
            "Epoch 56/200, Cost: 0.010165647432458961\n",
            "Epoch 57/200, Cost: 0.010207334450831862\n",
            "Epoch 58/200, Cost: 0.009967663561777522\n",
            "Epoch 59/200, Cost: 0.009931121177371998\n",
            "Epoch 60/200, Cost: 0.009888620341826212\n",
            "Epoch 61/200, Cost: 0.009860423094552304\n",
            "Epoch 62/200, Cost: 0.009633473996731822\n",
            "Epoch 63/200, Cost: 0.009600282145980655\n",
            "Epoch 64/200, Cost: 0.009566725757057437\n",
            "Epoch 65/200, Cost: 0.009448155707277575\n",
            "Epoch 66/200, Cost: 0.009591046564337269\n",
            "Epoch 67/200, Cost: 0.009438661765951867\n",
            "Epoch 68/200, Cost: 0.009218234250854544\n",
            "Epoch 69/200, Cost: 0.009255869151048292\n",
            "Epoch 70/200, Cost: 0.009228538149745545\n",
            "Epoch 71/200, Cost: 0.00917873669459842\n",
            "Epoch 72/200, Cost: 0.009047998132523643\n",
            "Epoch 73/200, Cost: 0.008915106959445376\n",
            "Epoch 74/200, Cost: 0.008808525372265799\n",
            "Epoch 75/200, Cost: 0.008856493349704092\n",
            "Epoch 76/200, Cost: 0.00906263746315672\n",
            "Epoch 77/200, Cost: 0.008827390419025695\n",
            "Epoch 78/200, Cost: 0.008798199417178139\n",
            "Epoch 79/200, Cost: 0.008790675463833126\n",
            "Epoch 80/200, Cost: 0.008483795827526338\n",
            "Epoch 81/200, Cost: 0.008582609595825585\n",
            "Epoch 82/200, Cost: 0.008847582568564537\n",
            "Epoch 83/200, Cost: 0.008529478002848787\n",
            "Epoch 84/200, Cost: 0.008400008737987092\n",
            "Epoch 85/200, Cost: 0.008299579674142174\n",
            "Epoch 86/200, Cost: 0.008211096521791483\n",
            "Epoch 87/200, Cost: 0.008295618112947099\n",
            "Epoch 88/200, Cost: 0.008224896450577234\n",
            "Epoch 89/200, Cost: 0.008059153308824254\n",
            "Epoch 90/200, Cost: 0.008032418974455535\n",
            "Epoch 91/200, Cost: 0.008291548614322138\n",
            "Epoch 92/200, Cost: 0.008013572623769\n",
            "Epoch 93/200, Cost: 0.008037691910850988\n",
            "Epoch 94/200, Cost: 0.0079473473909382\n",
            "Epoch 95/200, Cost: 0.007778997644100073\n",
            "Epoch 96/200, Cost: 0.007985702412337635\n",
            "Epoch 97/200, Cost: 0.007904650762439333\n",
            "Epoch 98/200, Cost: 0.00777610579392183\n",
            "Epoch 99/200, Cost: 0.0076470672488999905\n",
            "Epoch 100/200, Cost: 0.007727679358999232\n",
            "Epoch 101/200, Cost: 0.00786452516561692\n",
            "Epoch 102/200, Cost: 0.00764833741557667\n",
            "Epoch 103/200, Cost: 0.007529112609655622\n",
            "Epoch 104/200, Cost: 0.007542916385463143\n",
            "Epoch 105/200, Cost: 0.007371130982649646\n",
            "Epoch 106/200, Cost: 0.007504080509025533\n",
            "Epoch 107/200, Cost: 0.0077929399506324205\n",
            "Epoch 108/200, Cost: 0.007581974504903083\n",
            "Epoch 109/200, Cost: 0.007351941923607809\n",
            "Epoch 110/200, Cost: 0.00735747158303257\n",
            "Epoch 111/200, Cost: 0.007410974720899867\n",
            "Epoch 112/200, Cost: 0.0073936427474798495\n",
            "Epoch 113/200, Cost: 0.007359689279848649\n",
            "Epoch 114/200, Cost: 0.0073356651423486085\n",
            "Epoch 115/200, Cost: 0.007304215881831527\n",
            "Epoch 116/200, Cost: 0.007055912297645531\n",
            "Epoch 117/200, Cost: 0.007354266811652007\n",
            "Epoch 118/200, Cost: 0.006985586106150966\n",
            "Epoch 119/200, Cost: 0.007020251537491005\n",
            "Epoch 120/200, Cost: 0.006955840807884476\n",
            "Epoch 121/200, Cost: 0.006863194758303804\n",
            "Epoch 122/200, Cost: 0.006924284330043809\n",
            "Epoch 123/200, Cost: 0.00703213207898552\n",
            "Epoch 124/200, Cost: 0.0069727208356486\n",
            "Epoch 125/200, Cost: 0.006807403104977857\n",
            "Epoch 126/200, Cost: 0.006741574444408238\n",
            "Epoch 127/200, Cost: 0.00667429692995995\n",
            "Epoch 128/200, Cost: 0.006815556175157763\n",
            "Epoch 129/200, Cost: 0.007156696021965143\n",
            "Epoch 130/200, Cost: 0.006919938380989759\n",
            "Epoch 131/200, Cost: 0.006583952980059091\n",
            "Epoch 132/200, Cost: 0.006697393927484249\n",
            "Epoch 133/200, Cost: 0.006560751971130119\n",
            "Epoch 134/200, Cost: 0.0066944407928454215\n",
            "Epoch 135/200, Cost: 0.006575720494522973\n",
            "Epoch 136/200, Cost: 0.006752591196656347\n",
            "Epoch 137/200, Cost: 0.006542263657811522\n",
            "Epoch 138/200, Cost: 0.006475386658048489\n",
            "Epoch 139/200, Cost: 0.006437914776460344\n",
            "Epoch 140/200, Cost: 0.006528679561791936\n",
            "Epoch 141/200, Cost: 0.006558562013731587\n",
            "Epoch 142/200, Cost: 0.006670304156874873\n",
            "Epoch 143/200, Cost: 0.006355153246893216\n",
            "Epoch 144/200, Cost: 0.0064931944123349595\n",
            "Epoch 145/200, Cost: 0.006218065788437563\n",
            "Epoch 146/200, Cost: 0.006438220050177286\n",
            "Epoch 147/200, Cost: 0.006132783965861022\n",
            "Epoch 148/200, Cost: 0.006163161505341616\n",
            "Epoch 149/200, Cost: 0.0061862773014254415\n",
            "Epoch 150/200, Cost: 0.006172445680096398\n",
            "Epoch 151/200, Cost: 0.006705652046416349\n",
            "Epoch 152/200, Cost: 0.006029879206147673\n",
            "Epoch 153/200, Cost: 0.0062685945486533525\n",
            "Epoch 154/200, Cost: 0.00603969037404464\n",
            "Epoch 155/200, Cost: 0.006115242401287526\n",
            "Epoch 156/200, Cost: 0.006153356732299133\n",
            "Epoch 157/200, Cost: 0.005988601180633923\n",
            "Epoch 158/200, Cost: 0.005999667885352593\n",
            "Epoch 159/200, Cost: 0.006039928282635085\n",
            "Epoch 160/200, Cost: 0.006038807354317971\n",
            "Epoch 161/200, Cost: 0.005858980636439799\n",
            "Epoch 162/200, Cost: 0.00578953365614906\n",
            "Epoch 163/200, Cost: 0.005976555655964328\n",
            "Epoch 164/200, Cost: 0.0058400042300550264\n",
            "Epoch 165/200, Cost: 0.005939508053632829\n",
            "Epoch 166/200, Cost: 0.005718142978204332\n",
            "Epoch 167/200, Cost: 0.005721783613047901\n",
            "Epoch 168/200, Cost: 0.005736805459831482\n",
            "Epoch 169/200, Cost: 0.006044027035358672\n",
            "Epoch 170/200, Cost: 0.0061068022058835165\n",
            "Epoch 171/200, Cost: 0.00572062289922963\n",
            "Epoch 172/200, Cost: 0.005611350502305316\n",
            "Epoch 173/200, Cost: 0.0055345281663689875\n",
            "Epoch 174/200, Cost: 0.0056551346985069355\n",
            "Epoch 175/200, Cost: 0.005512342216497236\n",
            "Epoch 176/200, Cost: 0.005875858991350227\n",
            "Epoch 177/200, Cost: 0.005578731204808493\n",
            "Epoch 178/200, Cost: 0.0055803629035024085\n",
            "Epoch 179/200, Cost: 0.005817471689215384\n",
            "Epoch 180/200, Cost: 0.005529703260223078\n",
            "Epoch 181/200, Cost: 0.005377222941393464\n",
            "Epoch 182/200, Cost: 0.005446030129105472\n",
            "Epoch 183/200, Cost: 0.005487575915688989\n",
            "Epoch 184/200, Cost: 0.0055059336671564795\n",
            "Epoch 185/200, Cost: 0.005804483492882034\n",
            "Epoch 186/200, Cost: 0.005442132694639594\n",
            "Epoch 187/200, Cost: 0.0054725887231607175\n",
            "Epoch 188/200, Cost: 0.0052617804019780496\n",
            "Epoch 189/200, Cost: 0.005328603784844561\n",
            "Epoch 190/200, Cost: 0.0054142706354669235\n",
            "Epoch 191/200, Cost: 0.005257129121179292\n",
            "Epoch 192/200, Cost: 0.005315311226357453\n",
            "Epoch 193/200, Cost: 0.005333244778181527\n",
            "Epoch 194/200, Cost: 0.005317077315778718\n",
            "Epoch 195/200, Cost: 0.005331462169087952\n",
            "Epoch 196/200, Cost: 0.005109350262657124\n",
            "Epoch 197/200, Cost: 0.005393606724980053\n",
            "Epoch 198/200, Cost: 0.005035562065817645\n",
            "Epoch 199/200, Cost: 0.005363444751044448\n",
            "Epoch 200/200, Cost: 0.005343392993001033\n",
            "Total Cost: 0.005343392993001033\n",
            "Acurácia: 96.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 3 - Gradiente da função de custo"
      ],
      "metadata": {
        "id": "po9squAUI30d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    epsilon = 1e-8\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y_true * np.log(y_pred))\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.num_layers = len(hidden_sizes) + 1\n",
        "\n",
        "        sizes = [input_size] + hidden_sizes + [output_size]\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            self.weights.append(np.random.randn(sizes[i - 1], sizes[i]) * np.sqrt(2 / sizes[i - 1]))\n",
        "            self.biases.append(np.zeros((1, sizes[i])))\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        activations = [X]\n",
        "        for i in range(self.num_layers):\n",
        "            hidden_layer = relu(np.dot(activations[i], self.weights[i]) + self.biases[i])\n",
        "            activations.append(hidden_layer)\n",
        "        output_layer = softmax(np.dot(activations[-2], self.weights[-1]) + self.biases[-1])\n",
        "        return activations, output_layer\n",
        "\n",
        "    def train(self, X, y, epochs, batch_size):\n",
        "        num_examples = X.shape[0]\n",
        "        iterations_per_epoch = num_examples // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for _ in range(iterations_per_epoch):\n",
        "                indices = np.random.choice(num_examples, size=batch_size, replace=False)\n",
        "                X_batch = X[indices]\n",
        "                y_batch = y[indices]\n",
        "\n",
        "                activations, output_layer = self.forward_propagation(X_batch)\n",
        "\n",
        "                output_error = y_batch - output_layer\n",
        "                output_delta = output_error / batch_size\n",
        "\n",
        "                deltas = [output_delta]\n",
        "\n",
        "                for i in range(self.num_layers - 1, 0, -1):\n",
        "                    hidden_error = deltas[-1].dot(self.weights[i].T)\n",
        "                    hidden_delta = hidden_error * relu_derivative(activations[i])\n",
        "                    deltas.append(hidden_delta)\n",
        "\n",
        "                for i in range(self.num_layers - 1, -1, -1):\n",
        "                    self.weights[i] += activations[i].T.dot(deltas[self.num_layers - 1 - i]) * self.learning_rate\n",
        "                    self.biases[i] += np.sum(deltas[self.num_layers - 1 - i], axis=0) * self.learning_rate\n",
        "\n",
        "            _, output = self.forward_propagation(X)\n",
        "            cost = cross_entropy_loss(y, output)\n",
        "            \n",
        "\n",
        "        total_cost = cross_entropy_loss(y, output)\n",
        "        \n",
        "\n",
        "    def predict(self, X):\n",
        "        _, output = self.forward_propagation(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "    def calcular_gradiente(self, X, y):\n",
        "        activations, output_layer = self.forward_propagation(X)\n",
        "        output_error = y - output_layer\n",
        "        output_delta = output_error / X.shape[0]\n",
        "\n",
        "        deltas = [output_delta]\n",
        "\n",
        "        for i in range(self.num_layers - 1, 0, -1):\n",
        "            hidden_error = deltas[-1].dot(self.weights[i].T)\n",
        "            hidden_delta = hidden_error * relu_derivative(activations[i])\n",
        "            deltas.append(hidden_delta)\n",
        "\n",
        "        gradients = []\n",
        "        for i in range(self.num_layers - 1, -1, -1):\n",
        "            weight_gradient = activations[i].T.dot(deltas[self.num_layers - 1 - i])\n",
        "            bias_gradient = np.sum(deltas[self.num_layers - 1 - i], axis=0)\n",
        "            gradients.append((weight_gradient, bias_gradient))\n",
        "\n",
        "        return gradients\n",
        "\n",
        "\n",
        "# Carregando e pré-processando o conjunto de dados MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784) / 255.0\n",
        "X_test = X_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "y_train_encoded = np.zeros((y_train.shape[0], 10))\n",
        "y_train_encoded[np.arange(y_train.shape[0]), y_train] = 1\n",
        "\n",
        "# Definição e treinamento da MLP com ajustes\n",
        "mlp = MLP(input_size=784, hidden_sizes=[20, 20, 20], output_size=10, learning_rate=0.01)\n",
        "mlp.train(X_train, y_train_encoded, epochs=450, batch_size=550)\n",
        "\n",
        "# Calcular o gradiente da função custo\n",
        "gradients = mlp.calcular_gradiente(X_train, y_train_encoded)\n",
        "\n",
        "# Acessar os gradientes para cada camada\n",
        "for i, (weight_gradient, bias_gradient) in enumerate(gradients):\n",
        "    print(f\"Gradiente para a camada {i + 1}:\")\n",
        "    print(\"Peso:\")\n",
        "    print(weight_gradient)\n",
        "    print(\"Viés:\")\n",
        "    print(bias_gradient)\n",
        "    print()\n",
        "\n",
        "# Avaliação da MLP\n",
        "y_pred = mlp.predict(X_test)\n",
        "acuracia = np.mean(y_pred == y_test)\n",
        "print(\"Acurácia: {:.2%}\".format(acuracia))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BirVMt1LZDPi",
        "outputId": "4ff6fa2a-5ad2-45d1-8d8b-ff4a64e7fb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Gradiente para a camada 1:\n",
            "Peso:\n",
            "[[ 1.47180939e-05  1.50128194e-04 -8.55844176e-04  1.29481884e-03\n",
            "   1.43231211e-03 -2.93357438e-03  1.59305888e-03 -1.47697050e-03\n",
            "   7.51380583e-06  7.73839133e-04]\n",
            " [ 2.96450005e-04 -2.80708373e-04 -5.36603692e-04  5.83295758e-04\n",
            "   1.54115602e-04 -1.73611304e-03  1.99540915e-03 -7.29988629e-04\n",
            "   3.70625269e-04 -1.16482051e-04]\n",
            " [ 2.10572258e-04 -9.06435904e-05 -7.54872658e-04  2.34428676e-03\n",
            "   1.12513163e-03 -6.73545349e-03  4.52121504e-03 -1.70721181e-03\n",
            "  -1.41964667e-04  1.22894052e-03]\n",
            " [-2.76877684e-07 -4.77795320e-12 -1.35940364e-07 -1.66928104e-08\n",
            "   2.48101239e-06 -1.98601963e-08 -9.11346260e-08  4.16520002e-06\n",
            "  -1.45119356e-07 -5.96058259e-06]\n",
            " [ 1.07709602e-04  4.16240815e-06  4.27701834e-04  1.91531696e-05\n",
            "  -6.43505197e-06 -1.00418581e-04  8.33782673e-07 -3.45352393e-04\n",
            "  -9.39327823e-05 -1.34219885e-05]\n",
            " [ 1.11796225e-03 -9.14620404e-04 -8.76176279e-04  8.99947728e-04\n",
            "   4.12736840e-04 -4.73901220e-03  4.76505404e-03 -1.82809068e-03\n",
            "  -1.98759701e-04  1.36095841e-03]\n",
            " [-1.42496731e-04 -3.47743130e-04 -4.30051614e-04 -9.99529215e-05\n",
            "   6.79426539e-04 -1.77469466e-03  2.31658140e-03 -5.78305227e-04\n",
            "   4.91344229e-04 -1.14107890e-04]\n",
            " [-1.68093332e-04 -3.20488474e-04 -6.80666278e-04  9.78021435e-04\n",
            "   1.12350077e-03 -2.31782452e-03  1.39199855e-03 -9.26376593e-04\n",
            "   3.75972460e-04  5.43955981e-04]\n",
            " [ 4.20286971e-04 -3.05536802e-04 -1.36319685e-03  2.46282784e-03\n",
            "   8.72715735e-04 -5.87691226e-03  4.13284020e-03 -2.10110069e-03\n",
            "   4.97123173e-04  1.26095268e-03]\n",
            " [ 2.55748832e-04  2.31782699e-04 -3.05397750e-04  2.07401873e-03\n",
            "   1.45726736e-04 -1.44281637e-03  4.87175196e-04 -9.05159027e-04\n",
            "  -3.73650243e-04 -1.67428801e-04]\n",
            " [ 6.17297669e-04  1.38231664e-05 -6.62509170e-05 -3.54791903e-04\n",
            "   6.77836973e-05 -5.67345465e-04  1.12276619e-04  4.57739813e-05\n",
            "  -3.93904933e-05  1.70823645e-04]\n",
            " [ 2.84515221e-04 -1.03672070e-03 -3.74744593e-04  1.52403978e-03\n",
            "   2.01269151e-03 -2.85811384e-03  1.70923884e-03 -2.74362665e-03\n",
            "  -5.74301961e-04  2.05702240e-03]\n",
            " [ 2.52237908e-04 -3.07682129e-04 -7.43687089e-04  1.07834488e-03\n",
            "   2.51271929e-04 -4.47456350e-03  3.30781922e-03 -1.22304888e-03\n",
            "   3.93079408e-04  1.46622824e-03]\n",
            " [ 3.27732811e-04 -3.21964149e-05  4.19568238e-05 -1.44822897e-05\n",
            "  -2.31446360e-04 -1.14869423e-04  5.43662593e-05 -8.67249803e-05\n",
            "   6.54574958e-06  4.91178239e-05]\n",
            " [ 1.40912990e-04 -2.08722986e-04 -7.44767559e-05  9.19549502e-04\n",
            "   1.01335295e-03 -1.17799992e-03  4.06363485e-04 -2.01841958e-03\n",
            "  -1.89473881e-04  1.18891420e-03]\n",
            " [ 3.37153179e-04 -3.88083798e-04  4.89014004e-04  2.49095441e-03\n",
            "   2.51362452e-03 -4.31259592e-03  7.84842390e-04 -5.04234154e-03\n",
            "  -8.83651654e-05  3.21579792e-03]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 7.34788981e-04 -1.11696481e-04 -2.28293832e-04  1.80422786e-03\n",
            "   1.21542512e-03 -4.12784929e-03  1.48380233e-03 -2.77821964e-03\n",
            "  -2.09584471e-04  2.21739942e-03]\n",
            " [ 5.09076425e-04 -5.29204208e-04 -1.62669526e-03  4.28958888e-03\n",
            "   6.85424829e-04 -7.58121814e-03  3.11910304e-03 -2.70896513e-03\n",
            "   1.37924734e-03  2.46364222e-03]\n",
            " [ 2.28768198e-04 -8.81760978e-04 -7.81845198e-04  1.82865150e-03\n",
            "   2.20119964e-04 -2.67172026e-03  2.25172998e-03 -1.26781212e-03\n",
            "   9.26546638e-05  9.81214251e-04]]\n",
            "Viés:\n",
            "[ 9.34536514e-05 -3.26882571e-05 -3.25111740e-05  3.75233989e-04\n",
            "  1.15024351e-04 -8.21153528e-04  5.63055760e-04 -5.83289466e-04\n",
            "  2.02775886e-05  3.02597085e-04]\n",
            "\n",
            "Gradiente para a camada 2:\n",
            "Peso:\n",
            "[[ 1.97797709e-04 -4.49867204e-04  2.93517595e-04 -9.82391213e-11\n",
            "  -2.80982424e-04  1.14821713e-03  4.34959114e-04 -4.51697240e-04\n",
            "   7.13522391e-05 -5.46469027e-04  7.37201026e-05  5.74404598e-04\n",
            "   4.81989799e-04  1.01804303e-04  3.79717014e-04 -6.24235582e-04\n",
            "   0.00000000e+00 -4.53356719e-05 -1.04830686e-03  8.36394982e-05]\n",
            " [-7.35478158e-05 -4.84755000e-04  3.22767283e-04  1.79993866e-05\n",
            "  -6.55537998e-05  2.63731828e-04 -1.91648557e-04 -2.95806147e-04\n",
            "  -4.78514166e-04  1.08448496e-04  8.65154220e-05  4.87939169e-04\n",
            "   1.23568979e-04  6.75142542e-05  1.10803577e-04 -4.32658993e-04\n",
            "   0.00000000e+00 -1.00052576e-04  2.08523862e-04  6.90924600e-04]\n",
            " [ 4.30795153e-04  2.39447777e-04  7.62941953e-05 -2.76538136e-07\n",
            "  -4.83812834e-05 -6.69573406e-05  2.18185431e-04  1.13017580e-05\n",
            "  -2.00110671e-04  4.89786581e-04 -1.03096166e-04  8.66800228e-05\n",
            "  -1.12296486e-04 -1.77508990e-05 -4.48391035e-05 -3.79725633e-04\n",
            "   0.00000000e+00 -7.68702988e-04  2.44143948e-04  5.74034135e-04]\n",
            " [ 1.67812373e-03 -6.76450862e-04  1.38535049e-05  4.19067284e-05\n",
            "  -3.46462345e-04  2.28809639e-03 -4.19869037e-04 -1.15529478e-03\n",
            "  -1.44556308e-03  4.82528444e-04  9.95534706e-04  2.00976750e-03\n",
            "  -5.84939008e-04  1.28347763e-04 -1.18370776e-04 -2.14634660e-03\n",
            "   0.00000000e+00 -1.41433664e-03 -2.38989549e-04  2.69670066e-03]\n",
            " [ 3.09383898e-03 -1.81063716e-04 -1.06301516e-03  7.98468147e-06\n",
            "  -4.31422725e-04  3.46183150e-03  3.85692935e-04 -1.35353043e-03\n",
            "  -1.81073461e-03 -4.32243642e-04  7.28272459e-04  2.95968365e-03\n",
            "  -5.13714908e-04 -1.79568724e-06 -7.88604716e-04 -2.45613266e-03\n",
            "   0.00000000e+00 -2.65694560e-03 -3.55887082e-04  3.49430644e-03]\n",
            " [ 4.20827318e-03 -2.00918761e-04  2.71507117e-04  3.52539116e-05\n",
            "  -8.91650100e-04  6.24452449e-03  1.33750316e-03 -1.52592051e-03\n",
            "  -2.23563864e-03 -1.08284019e-03  9.13454533e-04  3.95386222e-03\n",
            "   4.03112019e-04  4.66942414e-04 -4.69518843e-04 -4.39851683e-03\n",
            "   0.00000000e+00 -3.64814208e-03 -3.12702470e-03  4.29761243e-03]\n",
            " [ 3.68051146e-04  3.12854331e-04  7.73179839e-04 -7.59254497e-07\n",
            "  -1.93198622e-04  7.39114724e-04  4.59094380e-04  1.03189095e-04\n",
            "   1.66294082e-04  1.75387407e-04  4.54913700e-05 -2.18289296e-04\n",
            "   7.51437391e-05  2.11598839e-04  3.95145483e-04 -9.63483004e-04\n",
            "   0.00000000e+00 -4.11344066e-04 -1.18276471e-03  5.09880782e-05]\n",
            " [ 3.02193092e-03 -1.01231589e-03  3.55960132e-04  2.30595359e-06\n",
            "  -6.95508175e-04  3.14698395e-03  6.60880860e-04 -7.69904283e-04\n",
            "  -1.22254907e-03 -4.79630636e-04  3.30915968e-04  2.24174443e-03\n",
            "  -4.78646473e-04  1.39250593e-04  1.31700921e-04 -2.83945287e-03\n",
            "   0.00000000e+00 -2.08993224e-03 -7.43263017e-04  2.33907730e-03]\n",
            " [ 8.85154570e-04 -6.03742183e-04  3.96973470e-04  1.03791648e-05\n",
            "  -7.81858661e-04  3.25393643e-03  5.90175668e-04 -7.70919383e-04\n",
            "  -1.02246387e-03 -1.09629537e-03  4.67608211e-04  1.36661239e-03\n",
            "   1.03264063e-03  3.99587335e-04 -2.11339615e-04 -2.55474160e-03\n",
            "   0.00000000e+00 -1.57653138e-03 -5.74012337e-04  2.29234295e-03]\n",
            " [ 4.84575080e-03 -7.44927835e-04 -6.85284825e-04  2.53297428e-05\n",
            "  -8.83519315e-04  4.89793588e-03  1.18935803e-03 -1.62321031e-03\n",
            "  -2.01491950e-03 -5.48906012e-04  9.05577163e-04  4.08289510e-03\n",
            "  -6.02035154e-04  3.29128183e-04 -1.31921541e-04 -3.79651489e-03\n",
            "   0.00000000e+00 -3.89885807e-03 -1.26139402e-03  4.27796794e-03]\n",
            " [ 3.58217376e-03 -3.96875026e-04 -7.90028481e-04  6.07680569e-05\n",
            "  -6.14042265e-04  4.96493329e-03  3.30137518e-04 -1.43930324e-03\n",
            "  -2.74671250e-03 -4.43652985e-04  1.51776951e-03  3.78582318e-03\n",
            "  -3.62158770e-04  1.45436424e-04 -1.13555346e-03 -3.28722076e-03\n",
            "   0.00000000e+00 -3.25588227e-03 -8.88273097e-04  4.52982395e-03]\n",
            " [ 5.33873366e-03 -8.33333068e-04 -2.17187213e-04  5.04041934e-05\n",
            "  -1.71885261e-03  5.48690828e-03  1.21022616e-03 -2.42502370e-03\n",
            "  -2.84587371e-03 -2.58284646e-04  1.29543004e-03  5.51755427e-03\n",
            "  -1.45027550e-04  5.56035116e-04  1.18762000e-03 -4.46300122e-03\n",
            "   0.00000000e+00 -4.22009393e-03 -7.26797179e-04  5.09300045e-03]\n",
            " [ 1.15022847e-03  1.38009624e-04 -6.38166185e-04  1.30728100e-06\n",
            "  -7.97979020e-05  5.91557468e-04  7.47223243e-05 -4.04051251e-04\n",
            "  -7.90602012e-04  2.32334834e-04  2.36562120e-04  1.16159644e-03\n",
            "  -5.67193714e-04 -1.71668542e-05 -4.72644902e-04 -1.95218363e-04\n",
            "   0.00000000e+00 -9.67644974e-04  1.87445287e-04  1.20898173e-03]\n",
            " [ 1.58785542e-03 -2.21561645e-04  6.53567987e-04  1.44781318e-05\n",
            "  -5.59272587e-04  2.33770046e-03  4.36117773e-04 -2.58294714e-04\n",
            "  -7.26039446e-04 -2.82048518e-04  3.96212897e-04  9.49278010e-04\n",
            "   1.68773529e-04  3.99561992e-04  4.25985431e-04 -2.16573925e-03\n",
            "   0.00000000e+00 -1.50618536e-03 -8.20559768e-04  1.35183243e-03]\n",
            " [ 4.83385905e-04 -3.56455771e-04  2.63558153e-04  2.70234251e-06\n",
            "  -2.18703365e-04  6.86709350e-04  2.39194939e-04 -6.47105332e-04\n",
            "  -3.65037503e-05  3.05658138e-04 -4.22707848e-05  5.64661401e-04\n",
            "  -2.98871179e-05  1.90753688e-04  5.24629547e-04 -1.06819097e-03\n",
            "   0.00000000e+00 -6.68133246e-04  3.07400088e-04  1.06199763e-03]\n",
            " [ 9.68518825e-04 -3.05600960e-04 -3.71151022e-04  2.07826643e-06\n",
            "  -3.82892246e-05  9.89164769e-04  1.17810774e-04 -7.25300768e-04\n",
            "  -2.86703333e-04  4.10339492e-04  2.62468102e-04  1.09543031e-03\n",
            "  -4.61319954e-04  1.92948695e-05  3.77406412e-04 -6.86902290e-04\n",
            "   0.00000000e+00 -3.86817093e-04 -3.01111890e-04  8.24302276e-04]\n",
            " [ 6.92789876e-05 -7.39511753e-06 -1.27752471e-05  0.00000000e+00\n",
            "   8.06039450e-06 -2.79363599e-05  3.24362856e-05  1.76882541e-05\n",
            "  -1.18709933e-05  2.63250403e-05 -5.35598574e-06  5.13486605e-05\n",
            "  -5.37086059e-05 -1.51463679e-05  1.04118923e-05  2.48983425e-05\n",
            "   0.00000000e+00 -6.58411537e-07 -6.57767716e-05 -1.35410079e-05]\n",
            " [ 1.82230997e-03 -9.69086608e-05 -7.35090325e-04  3.63001703e-06\n",
            "  -8.55713294e-05  1.98982915e-03 -5.12328280e-05 -7.54714088e-04\n",
            "  -1.19698034e-03  8.73900291e-05  5.59325794e-04  1.67120664e-03\n",
            "  -6.92798384e-04 -9.40161638e-06 -7.52469468e-04 -1.61484478e-03\n",
            "   0.00000000e+00 -1.59949497e-03  2.37183781e-04  2.32915826e-03]\n",
            " [ 1.21575217e-04  1.19276784e-04 -5.73884499e-05  2.78771884e-07\n",
            "  -1.02480427e-05  1.96036703e-04 -1.90426723e-05 -3.45673765e-04\n",
            "  -4.77800450e-05  9.94166176e-05  1.55756736e-05  2.63764199e-04\n",
            "  -3.24755563e-05  9.54095133e-05  3.10963053e-05 -1.68878796e-04\n",
            "   0.00000000e+00 -1.36128901e-04 -1.65589839e-04  2.54565544e-04]\n",
            " [ 2.10210010e-04  1.64400510e-04  3.72855651e-04  2.37104558e-06\n",
            "  -3.72248374e-04  1.72139049e-03  3.48791112e-04 -4.38966240e-04\n",
            "  -3.01542412e-04 -1.31467089e-04  3.39550698e-04  1.66060747e-04\n",
            "   6.59467082e-04  3.58567755e-04 -5.77614031e-05 -1.55770796e-03\n",
            "   0.00000000e+00 -8.77304685e-04 -7.63657099e-04  9.65211924e-04]]\n",
            "Viés:\n",
            "[ 7.99661116e-04 -1.44680183e-04 -3.00533101e-05  7.14386119e-06\n",
            " -1.63109235e-04  1.24848164e-03  4.14898163e-04 -4.47081065e-04\n",
            " -4.38792175e-04 -6.74852357e-05  2.97825211e-04  7.57022173e-04\n",
            "  7.56086493e-05  1.11204632e-04 -9.34461482e-05 -9.80428863e-04\n",
            "  0.00000000e+00 -8.75935248e-04 -3.18534631e-04  1.05069339e-03]\n",
            "\n",
            "Gradiente para a camada 3:\n",
            "Peso:\n",
            "[[ 1.19868564e-03 -7.45218229e-06 -1.44549315e-03  7.89090894e-04\n",
            "  -8.67933880e-04  2.41314168e-03  2.35360755e-03  1.48912664e-03\n",
            "   3.29287873e-03 -1.85111267e-03  1.76860301e-03 -3.40630792e-03\n",
            "  -1.59365359e-03 -2.20603171e-04 -1.55844141e-03 -1.70620463e-04\n",
            "   2.92153288e-05 -9.30125156e-04 -3.73459290e-04 -1.17530413e-03]\n",
            " [ 2.49131520e-04  3.15405001e-04 -6.70188465e-04  9.80254300e-04\n",
            "  -5.16616392e-04  9.97783069e-04  1.02584238e-03  1.75144491e-03\n",
            "   1.04222241e-03 -1.82927295e-03  8.26479976e-04 -5.90192819e-04\n",
            "   7.72012082e-06 -5.25952123e-04 -6.62198426e-04  3.98235154e-04\n",
            "   3.83900658e-05 -6.46494603e-04  5.31452537e-05 -7.39774627e-04]\n",
            " [ 1.41219216e-04 -1.48487884e-04 -5.54406247e-04  1.98270237e-04\n",
            "  -1.98689632e-04  5.40646060e-04  2.73422512e-04  5.83135141e-04\n",
            "   1.58304849e-03 -4.13411221e-04  3.33438115e-04 -1.20920802e-03\n",
            "  -3.77416777e-04 -3.53832962e-04 -2.60434239e-04 -2.94424551e-04\n",
            "   6.82228537e-05  4.76511409e-05 -7.51351616e-05 -5.41766401e-04]\n",
            " [ 1.78132975e-04 -1.59600897e-04  2.41117153e-05  3.87687405e-04\n",
            "   9.49719269e-06  2.27648241e-04  4.02223287e-04  6.81030941e-04\n",
            "   3.66164696e-05 -5.94442339e-04  2.58392996e-04 -4.00866181e-04\n",
            "   9.04080446e-05 -6.56668273e-05 -3.56139011e-04  2.70196199e-04\n",
            "   3.70126515e-06 -2.58238135e-04 -4.48700262e-05  2.53278396e-05]\n",
            " [ 1.20383470e-03 -7.50099884e-04 -7.21463088e-04  6.21488611e-04\n",
            "  -8.68097872e-04  1.71193683e-03  1.79117531e-03  6.90858986e-04\n",
            "   2.33842548e-03 -2.28071368e-04  5.24775524e-04 -1.11186446e-03\n",
            "  -1.58151100e-03  2.99451986e-04 -1.54548614e-03 -5.19119424e-05\n",
            "   4.10832221e-05 -1.80732423e-03 -5.64667602e-04 -1.86976672e-03]\n",
            " [ 6.27391022e-04 -1.43594801e-04 -1.16045253e-03  1.03288378e-03\n",
            "  -1.26186632e-04  1.56938067e-03  6.23484728e-04  6.06971873e-04\n",
            "   1.75455050e-03 -1.61179920e-03  4.47487656e-04 -1.37484656e-03\n",
            "  -1.00121314e-03 -1.15741790e-03 -7.39112861e-04  1.04489453e-04\n",
            "   1.50833795e-07 -7.88596576e-04 -5.29671313e-05 -7.00953306e-04]\n",
            " [ 1.29090940e-03  5.05019214e-04 -2.13770775e-03  1.34228158e-03\n",
            "  -2.97595516e-04  3.29715172e-03  2.42742947e-03  1.74592788e-03\n",
            "   4.54492583e-03 -2.80469662e-03  2.38419080e-03 -2.50383640e-03\n",
            "  -1.34741668e-03 -1.83261736e-03 -2.27612229e-03  3.06082540e-05\n",
            "  -7.28012582e-05 -1.97253439e-03 -2.55586154e-04 -2.14772071e-03]\n",
            " [ 4.15005445e-04 -3.34045575e-04 -4.15110573e-04  5.44233936e-04\n",
            "  -2.73021696e-04  7.04145560e-04 -4.58732728e-05  5.30884313e-04\n",
            "   1.44545848e-03 -7.67299141e-04  2.03494224e-04 -1.07184778e-03\n",
            "  -7.41412639e-04 -6.42427673e-04 -1.85918245e-04  3.38665013e-04\n",
            "   4.66157376e-05 -2.73040024e-04  9.43657217e-05 -1.06213212e-04]\n",
            " [ 7.61850599e-04  6.64623539e-04 -4.02404206e-04  7.59995630e-04\n",
            "  -1.27126395e-03  1.02167525e-03  1.36604789e-03  6.31550492e-04\n",
            "   1.67978686e-03 -1.31801443e-03  1.67379605e-03 -1.59193396e-03\n",
            "  -8.41916883e-04  4.43654102e-04 -7.24536251e-04  6.79237060e-05\n",
            "  -1.71676947e-05 -4.68112225e-04 -1.60643758e-04 -8.95373278e-04]\n",
            " [ 7.01010939e-04 -6.10908273e-05 -5.76291442e-04  8.61283034e-04\n",
            "  -1.76534709e-03  2.82341721e-03  2.09992443e-03  2.34238221e-04\n",
            "   1.98184493e-03 -1.85453365e-03  2.42736369e-03 -2.58978914e-03\n",
            "  -7.04519771e-04  1.70564875e-04 -1.57122074e-03  9.97767056e-04\n",
            "  -2.23263554e-05 -8.76745182e-04 -2.41496193e-04 -1.33699194e-03]\n",
            " [ 4.41907150e-04 -1.98012094e-04 -2.56071207e-04  1.70609467e-04\n",
            "   1.77860068e-04  2.41890092e-03  1.14680034e-03  8.30012083e-04\n",
            "   2.39106301e-03 -1.13971729e-03  1.17405509e-03 -2.03389904e-03\n",
            "  -3.64855655e-04 -8.20994003e-04 -1.79289309e-03  7.43266748e-05\n",
            "  -1.14795750e-05 -6.31284362e-04 -5.34184020e-04 -1.34389824e-03]\n",
            " [ 1.08190502e-03  2.52007320e-04 -9.49727240e-04 -2.82159598e-04\n",
            "  -8.14897636e-04  1.96440022e-03  2.29832462e-03  6.43942593e-04\n",
            "   1.76198832e-03 -8.30668761e-04  1.42698962e-03 -1.30617002e-03\n",
            "  -7.81033476e-04  5.30490406e-04 -1.58396779e-03 -2.78136916e-04\n",
            "   9.63059233e-06 -1.11341126e-03 -4.84162758e-04 -1.51489912e-03]\n",
            " [-2.25018928e-04 -3.04343115e-04 -4.58994729e-04  5.80528683e-04\n",
            "   6.71136091e-04 -5.90098889e-05 -1.57571747e-04  3.56873836e-04\n",
            "   1.67850394e-03 -5.22464770e-05 -6.17899486e-05 -2.28153377e-04\n",
            "  -6.92597327e-04 -1.19744926e-03 -6.67050473e-05  3.93529967e-06\n",
            "   2.58610893e-05 -2.27611872e-04  9.40433742e-05 -3.96895711e-04]\n",
            " [ 2.10887989e-03 -2.75697545e-04 -1.63367962e-03  1.31072916e-03\n",
            "  -1.95567273e-03  5.60560217e-03  3.77512537e-03  8.34254290e-04\n",
            "   4.54355652e-03 -3.42164839e-03  2.59010057e-03 -4.45934292e-03\n",
            "  -2.62509655e-03 -8.62622033e-04 -3.06058371e-03  1.21074344e-03\n",
            "  -4.17449457e-05 -2.70588134e-03 -4.64041037e-04 -1.75212343e-03]\n",
            " [ 2.24335101e-03 -1.05017651e-03 -4.58820781e-03  3.52499128e-03\n",
            "  -1.62128816e-03  5.12387099e-03  3.55850515e-03  5.06842349e-03\n",
            "   9.48919772e-03 -4.02793466e-03  2.82078750e-03 -4.92931018e-03\n",
            "  -5.03075476e-03 -2.31686489e-03 -3.92780220e-03  1.58146243e-04\n",
            "   1.05606947e-04 -4.91390888e-03 -1.11282859e-03 -4.19802858e-03]\n",
            " [ 1.48046451e-04  5.47480492e-04 -1.14263360e-03  2.71305822e-04\n",
            "   5.06439487e-04  7.92963533e-04  5.72572892e-04  1.90937731e-03\n",
            "   1.29112427e-03 -1.05453148e-03  3.34275110e-05 -3.22905303e-04\n",
            "  -4.74241887e-04 -1.24236368e-03 -7.00816277e-04  1.96932141e-04\n",
            "  -3.45890560e-05 -6.84697505e-04 -2.10597737e-04 -1.02125133e-03]\n",
            " [ 2.88900354e-03 -4.97903144e-04 -2.95390151e-03  2.77658644e-03\n",
            "  -2.35054455e-03  8.50431387e-03  4.70723738e-03  1.97963581e-03\n",
            "   7.86765422e-03 -5.15999631e-03  3.60610163e-03 -5.85890590e-03\n",
            "  -3.93861117e-03 -2.08565408e-03 -4.61331871e-03  8.37559472e-04\n",
            "   3.27410398e-05 -4.11674920e-03 -9.10830092e-04 -3.45598790e-03]\n",
            " [ 8.01959035e-04  2.27940068e-04 -2.20734872e-03  8.28706971e-04\n",
            "   2.28272685e-04  3.90979779e-03  2.22646606e-03  7.58449303e-04\n",
            "   6.05733106e-03 -2.53431796e-03  2.00284945e-03 -3.69412682e-03\n",
            "  -2.36445668e-03 -2.92233292e-03 -2.18039362e-03 -1.18483338e-04\n",
            "  -4.19053702e-05 -1.73227490e-03 -1.54760233e-04 -1.56130278e-03]\n",
            " [ 1.84678756e-06 -2.55060485e-04 -1.26097036e-03  9.40573358e-04\n",
            "  -5.45639272e-04 -3.04126727e-04  5.57498302e-04  1.62366113e-03\n",
            "   2.05388808e-03 -3.38162196e-04  9.12018880e-04 -1.40172165e-04\n",
            "  -6.95197096e-04  7.16269618e-05 -3.60923648e-04 -1.69342445e-04\n",
            "   9.52209864e-05 -1.24975299e-03 -3.46307105e-04 -1.47540362e-03]\n",
            " [ 1.35700939e-03  1.95270323e-04 -3.39242517e-04  1.87604908e-04\n",
            "  -1.16724643e-03  3.59871910e-03  2.98401942e-03  4.07213879e-05\n",
            "   2.74391472e-03 -1.92118324e-03  2.21325285e-03 -3.45793704e-03\n",
            "  -1.90101270e-03  5.30182267e-05 -2.04782633e-03 -9.42144677e-05\n",
            "  -1.09958554e-05 -1.10238913e-03 -6.10590639e-04 -1.02501515e-03]]\n",
            "Viés:\n",
            "[ 6.47155149e-04 -7.87104460e-05 -8.98099431e-04  7.22258900e-04\n",
            " -3.27186513e-04  2.04141229e-03  1.48598323e-03  9.27685549e-04\n",
            "  2.75261631e-03 -1.38023032e-03  1.23267613e-03 -2.07390648e-03\n",
            " -1.44773401e-03 -8.26463087e-04 -1.23302861e-03  6.71804753e-05\n",
            "  4.35563300e-05 -1.13803917e-03 -3.48011968e-04 -1.13023098e-03]\n",
            "\n",
            "Gradiente para a camada 4:\n",
            "Peso:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Viés:\n",
            "[-0.00101423  0.00129092 -0.00393457 -0.00300646 -0.00199037  0.00013523\n",
            " -0.00147522 -0.00124239 -0.00201543 -0.0004075  -0.00234972 -0.00415414\n",
            " -0.00161922  0.00108144 -0.00166225 -0.00199821  0.00488707  0.00481428\n",
            " -0.00203358  0.00248006]\n",
            "\n",
            "Acurácia: 96.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARTE 4 - Retropropagação"
      ],
      "metadata": {
        "id": "aoCWnKfPqhn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    epsilon = 1e-8\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y_true * np.log(y_pred))\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights1 = np.random.randn(self.input_size, self.hidden_sizes[0]) * np.sqrt(2 / self.input_size)\n",
        "        self.weights2 = np.random.randn(self.hidden_sizes[0], self.hidden_sizes[1]) * np.sqrt(2 / self.hidden_sizes[0])\n",
        "        self.weights3 = np.random.randn(self.hidden_sizes[1], self.output_size) * np.sqrt(2 / self.hidden_sizes[1])\n",
        "\n",
        "        self.biases1 = np.zeros((1, self.hidden_sizes[0]))\n",
        "        self.biases2 = np.zeros((1, self.hidden_sizes[1]))\n",
        "        self.biases3 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        hidden_layer1 = relu(np.dot(X, self.weights1) + self.biases1)\n",
        "        hidden_layer2 = relu(np.dot(hidden_layer1, self.weights2) + self.biases2)\n",
        "        output_layer = softmax(np.dot(hidden_layer2, self.weights3) + self.biases3)\n",
        "        return hidden_layer1, hidden_layer2, output_layer\n",
        "\n",
        "    def backward_propagation(self, X, y, hidden_layer1, hidden_layer2, output_layer):\n",
        "        batch_size = X.shape[0]\n",
        "\n",
        "        output_error = output_layer - y\n",
        "        output_delta = output_error / batch_size\n",
        "\n",
        "        hidden_error2 = output_delta.dot(self.weights3.T)\n",
        "        hidden_delta2 = hidden_error2 * relu_derivative(hidden_layer2)\n",
        "\n",
        "        hidden_error1 = hidden_delta2.dot(self.weights2.T)\n",
        "        hidden_delta1 = hidden_error1 * relu_derivative(hidden_layer1)\n",
        "\n",
        "        self.weights3 -= hidden_layer2.T.dot(output_delta) * self.learning_rate\n",
        "        self.weights2 -= hidden_layer1.T.dot(hidden_delta2) * self.learning_rate\n",
        "        self.weights1 -= X.T.dot(hidden_delta1) * self.learning_rate\n",
        "\n",
        "        self.biases3 -= np.sum(output_delta, axis=0) * self.learning_rate\n",
        "        self.biases2 -= np.sum(hidden_delta2, axis=0) * self.learning_rate\n",
        "        self.biases1 -= np.sum(hidden_delta1, axis=0) * self.learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, batch_size):\n",
        "        num_examples = X.shape[0]\n",
        "        iterations_per_epoch = num_examples // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for _ in range(iterations_per_epoch):\n",
        "                indices = np.random.choice(num_examples, size=batch_size, replace=False)\n",
        "                X_batch = X[indices]\n",
        "                y_batch = y[indices]\n",
        "\n",
        "                hidden_layer1, hidden_layer2, output_layer = self.forward_propagation(X_batch)\n",
        "\n",
        "                self.backward_propagation(X_batch, y_batch, hidden_layer1, hidden_layer2, output_layer)\n",
        "\n",
        "            _, _, output = self.forward_propagation(X)\n",
        "            cost = cross_entropy_loss(y, output)\n",
        "\n",
        "            \n",
        "\n",
        "    def predict(self, X):\n",
        "        _, _, output = self.forward_propagation(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "# Loading and preprocessing the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784) / 255.0\n",
        "X_test = X_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "y_train_encoded = np.zeros((y_train.shape[0], 10))\n",
        "y_train_encoded[np.arange(y_train.shape[0]), y_train] = 1\n",
        "\n",
        "# Definition and training of the MLP\n",
        "mlp = MLP(input_size=784, hidden_sizes=[20, 20], output_size=10, learning_rate=0.01)\n",
        "mlp.train(X_train, y_train_encoded, epochs=500, batch_size=200)\n",
        "\n",
        "# Evaluation of the MLP\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZcX2Pz-AjA-",
        "outputId": "8ce1ff77-e877-441d-e816-e806b8aae938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 96.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 5"
      ],
      "metadata": {
        "id": "1U3AzJ2VyiOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    epsilon = 1e-8\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y_true * np.log(y_pred))\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Inicialização dos pesos e bias das camadas ocultas\n",
        "        for i in range(len(hidden_sizes)):\n",
        "            if i == 0:\n",
        "                self.weights.append(np.random.randn(self.input_size, self.hidden_sizes[0]) * np.sqrt(2 / self.input_size))\n",
        "                self.biases.append(np.zeros((1, self.hidden_sizes[0])))\n",
        "            else:\n",
        "                self.weights.append(np.random.randn(self.hidden_sizes[i-1], self.hidden_sizes[i]) * np.sqrt(2 / self.hidden_sizes[i-1]))\n",
        "                self.biases.append(np.zeros((1, self.hidden_sizes[i])))\n",
        "\n",
        "        # Inicialização dos pesos e bias da camada de saída\n",
        "        self.weights.append(np.random.randn(self.hidden_sizes[-1], self.output_size) * np.sqrt(2 / self.hidden_sizes[-1]))\n",
        "        self.biases.append(np.zeros((1, self.output_size)))\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        hidden_layers = [relu(np.dot(X, self.weights[0]) + self.biases[0])]\n",
        "        \n",
        "        for i in range(1, len(self.hidden_sizes)):\n",
        "            hidden_layers.append(relu(np.dot(hidden_layers[i-1], self.weights[i]) + self.biases[i]))\n",
        "        \n",
        "        output_layer = softmax(np.dot(hidden_layers[-1], self.weights[-1]) + self.biases[-1])\n",
        "        return hidden_layers, output_layer\n",
        "\n",
        "    def backward_propagation(self, X, y, hidden_layers, output_layer):\n",
        "        batch_size = X.shape[0]\n",
        "\n",
        "        output_error = output_layer - y\n",
        "        output_delta = output_error / batch_size\n",
        "\n",
        "        hidden_error = []\n",
        "        hidden_delta = []\n",
        "\n",
        "        hidden_error.append(output_delta.dot(self.weights[-1].T))\n",
        "        hidden_delta.append(hidden_error[0] * relu_derivative(hidden_layers[-1]))\n",
        "\n",
        "        for i in range(len(self.hidden_sizes) - 2, -1, -1):\n",
        "            hidden_error.insert(0, hidden_delta[0].dot(self.weights[i+1].T))\n",
        "            hidden_delta.insert(0, hidden_error[0] * relu_derivative(hidden_layers[i]))\n",
        "\n",
        "        for i in range(len(self.hidden_sizes)):\n",
        "            if i == 0:\n",
        "                self.weights[i] -= X.T.dot(hidden_delta[i]) * self.learning_rate\n",
        "            else:\n",
        "                self.weights[i] -= hidden_layers[i-1].T.dot(hidden_delta[i]) * self.learning_rate\n",
        "        \n",
        "        self.weights[-1] -= hidden_layers[-1].T.dot(output_delta) * self.learning_rate\n",
        "\n",
        "        for i in range(len(self.hidden_sizes)):\n",
        "            self.biases[i] -= np.sum(hidden_delta[i], axis=0) * self.learning_rate\n",
        "        \n",
        "        self.biases[-1] -= np.sum(output_delta, axis=0) * self.learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, batch_size):\n",
        "        num_examples = X.shape[0]\n",
        "        iterations_per_epoch = num_examples // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for _ in range(iterations_per_epoch):\n",
        "                indices = np.random.choice(num_examples, size=batch_size, replace=False)\n",
        "                X_batch = X[indices]\n",
        "                y_batch = y[indices]\n",
        "\n",
        "                hidden_layers, output_layer = self.forward_propagation(X_batch)\n",
        "\n",
        "                self.backward_propagation(X_batch, y_batch, hidden_layers, output_layer)\n",
        "\n",
        "            _, output = self.forward_propagation(X)\n",
        "            cost = cross_entropy_loss(y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        hidden_layers, output = self.forward_propagation(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "# Loading and preprocessing the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784) / 255.0\n",
        "X_test = X_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "y_train_encoded = np.zeros((y_train.shape[0], 10))\n",
        "y_train_encoded[np.arange(y_train.shape[0]), y_train] = 1\n",
        "\n",
        "# Test different numbers of hidden layers\n",
        "hidden_layer_sizes = [[20], [20, 20], [20, 20, 20], [20,20,20,20]]\n",
        "\n",
        "for hidden_sizes in hidden_layer_sizes:\n",
        "    mlp = MLP(input_size=784, hidden_sizes=hidden_sizes, output_size=10, learning_rate=0.01)\n",
        "    mlp.train(X_train, y_train_encoded, epochs=250, batch_size=126)\n",
        "\n",
        "    y_pred = mlp.predict(X_test)\n",
        "    accuracy = np.mean(y_pred == y_test)\n",
        "    print(\"Hidden Layers: {}, Accuracy: {:.2%}\".format(len(hidden_sizes), accuracy))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oawA1I0Tyjl8",
        "outputId": "0656263e-1249-4968-f69c-8a35e788911d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Layers: 1, Accuracy: 95.91%\n",
            "Hidden Layers: 2, Accuracy: 96.00%\n",
            "Hidden Layers: 3, Accuracy: 96.02%\n",
            "Hidden Layers: 4, Accuracy: 95.93%\n"
          ]
        }
      ]
    }
  ]
}